# Libraries
import base64
import pandas as pd
import numpy as np
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, LatentDirichletAllocation

# Plotting libraries
import plotly.offline as py
py.init_notebook_mode(connected = True)
import plotly.graph_objs as go
import plotly.tools as tls
from matplotlib import pyplot as plt
%matplotlib inline


# Import data
df = pd.read_csv('D:/work/pythonproject/newtonnlp/csv/fulldatasetcleaned.csv', encoding = "utf-8")
df = df.drop(df.columns[0], axis=1)
list(df)


# Calling for lemmatizer as stemming can be unreliable
lemm = WordNetLemmatizer()
# Combining sklearn vectorizer with lemmatization
# Subclass the original CountVectorizer, change the build_analyzer method to have lemmatization
class LemmaCountVectorizer(CountVectorizer):
    def build_analyzer(self):
        analyzer = super(LemmaCountVectorizer, self).build_analyzer()
        return lambda doc: (lemm.lemmatize(ianalyzer) for ianalyzer in analyzer(doc))
    
# Create a list of all the texts to be vectorized
text = list(df.full_text)
# Calling the vectorizer function above
tf_vectorizer = LemmaCountVectorizer(max_df = 0.95,
                                     min_df = 2,
                                     stop_words = 'english',
                                     decode_error = 'ignore')
tf = tf_vectorizer.fit_transform(text)


# Term frequency graphs
# List of features / words
feature_names = tf_vectorizer.get_feature_names()
# List of count of each word
count_vec = np.asarray(tf.sum(axis = 0)).ravel()
# Combining 2 lists together
zipped = list(zip(feature_names, count_vec))
x, y = (list(i) for i in zip(*sorted(zipped, key = lambda i: i[1], reverse=True)))
# Plotting
data = [go.Bar(x = x[0:50], y = y[0:50],
               marker = dict(colorscale = 'Jet', color = y[0:50]),
               text = 'Word Count')]
layout = go.Layout(title = 'Word Frequency - Top 50')
fig = go.Figure(data = data, layout = layout)    
py.plot(fig, filename = 'basic-bar.html')


# Top and bottom 15 words in the corpus
bottom = np.concatenate([y[0:15], y[-16:-1]])
top = np.concatenate([x[0:15], x[-16:-1]])


# Creating a topic model
# Applying sklearn implementation of LDA, genism's implementation is worth considering
# 10 topics, maximum 5 iterations, switch batch to online if data size too large, set seed for reproducability
lda = LatentDirichletAllocation(n_components = 10,
                                max_iter = 5,
                                learning_method = 'batch',
                                learning_offset = 50,
                                random_state = 0)
# Fitting LDA on vectorized text to generate topic models
lda.fit(tf)
# List of topics and top words in each topic
# Function to print out top words
def top_words(model, feature, n_words):
    for index, topic in enumerate(model.components_):                                   # Loop through the topics
        message = '\nTopic #{}:'.format(index)                                          # For each topic, number it
        message += ' '.join([feature[i] for i in topic.argsort()[:-n_words - 1 :-1]])   # and take the top used n_words
        print(message)
        print('='*20)
tf_feature_names = tf_vectorizer.get_feature_names()
# Printing out 40 words
n_words = 40
top_words(lda, tf_feature_names, n_words)                                               
# Frequency graphs for each topic models
# pyLDAvis
first_topic = lda.components_[0]





############ Topics are generated by only using religious text and with latin and english combined
############ Graphs and word clouds to be done, considering pyLDAvis for more topic models focused graphs


